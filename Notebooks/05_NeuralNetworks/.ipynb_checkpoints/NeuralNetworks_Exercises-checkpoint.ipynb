{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlpG_fIiqdJU"
   },
   "source": [
    "# Neural Network Exercise\n",
    "\n",
    "In this Exercise Notebook you will be building your own artificial neural network and seeing how adding different types of layers can affect the validation/testing accuracy. This is based off of the Simple Neural Network with Keras tutorial, so you can reference that for further explanations as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eFSzOvSduDf8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ep_09mlyqa5G"
   },
   "outputs": [],
   "source": [
    "os.system('wget https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/NotebookExampleData/Week2/spoken_digit_manual_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zChcGaqVysRB"
   },
   "source": [
    "## Load Training Data and Pre-processed Features\n",
    "\n",
    "Your goal is to build a neural network that learns to classify which of the 5 speakers is recorded in a signal sample. Your prediction will be based off of features we've already pre-extracted for you and put into this CSV: spectral centroid `SC`, spectral flatness `SF`, and maximum frequency `MF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SVwsmOGvw7jp"
   },
   "outputs": [],
   "source": [
    "# Load csv containing raw data, labels, and pre-processed features\n",
    "spoken_df = pd.read_csv('spoken_digit_manual_features.csv', index_col = 0)\n",
    "print(spoken_df.head(10))\n",
    "print('\\n')\n",
    "\n",
    "# Set speakers\n",
    "speakers = set(spoken_df['speaker'])\n",
    "print(f'There are {len(speakers)} unique speakers in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mArY7lB4Akv1"
   },
   "source": [
    "Converting labels to \"onehot\" vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nLRtFkiYAc3N"
   },
   "outputs": [],
   "source": [
    "# Make dictionary to convert from speaker names to indices\n",
    "name2int_dict = {name: ind for (ind, name) in enumerate(set(spoken_df['speaker']))}\n",
    "\n",
    "y_labels = spoken_df['speaker']\n",
    "# Set y_labels to be indices of speaker\n",
    "y_labels = [name2int_dict[name] for name in y_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xUhDZMw1A93D"
   },
   "source": [
    "Standardize data and split into train, validation, and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TAuzw6ibA3Es"
   },
   "outputs": [],
   "source": [
    "# Downselect to only the 3 columns of the dataset we are learning from, aka the features\n",
    "X_data = spoken_df[['SC', 'SF', 'MF']].to_numpy()\n",
    "\n",
    "# Decide how large to make validation and test sets\n",
    "n_val = 250\n",
    "n_test = 250\n",
    "\n",
    "# Shuffle data before partitioning\n",
    "X_data, y_labels = shuffle(X_data, y_labels, random_state = 25)\n",
    "\n",
    "# Partition\n",
    "X_data_test, y_labels_test = X_data[:n_test,:], y_labels[:n_test]\n",
    "X_data_val, y_labels_val = X_data[n_test:n_test+n_val,:], y_labels[n_test:n_test+n_val]\n",
    "X_data_train, y_labels_train = X_data[n_test+n_val:,:], y_labels[n_test+n_val:]\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X_data_train=scaler.fit_transform(X_data_train)\n",
    "X_data_val = scaler.transform(X_data_val)\n",
    "X_data_test = scaler.transform(X_data_test)\n",
    "\n",
    "# Convert labels to onehot\n",
    "y_labels_train = tf.keras.utils.to_categorical(y_labels_train, 5)\n",
    "y_labels_val =  tf.keras.utils.to_categorical(y_labels_val, 5)\n",
    "y_labels_test =  tf.keras.utils.to_categorical(y_labels_test, 5)\n",
    "\n",
    "training_set = tf.data.Dataset.from_tensor_slices((X_data_train, y_labels_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "riycN8SdyxNT"
   },
   "source": [
    "## Aditional Layers\n",
    "\n",
    "Before you get to writing your own neural network we'll show you some examples of additional layers you can potetially add that we didn't go over in the tutorial. After reading over our explanations/example code and going through documentation you'll be testing some of these out by putting together a neural network yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DoaZsqc3iEyv"
   },
   "source": [
    "### Dropout Layers\n",
    "\n",
    "Dropout layers randomly omit, or drop, some elements of the output vector from the layer, which helps prevent overfitting and can improve the generalization of your neural network. The dropout rate can be any number between 0 and 1.\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout\n",
    "\n",
    "```python\n",
    "# Example\n",
    "d_r = 0.6\n",
    "tf.keras.layers.Dropout(rate=d_r)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVJMrZ09iUgk"
   },
   "source": [
    "### Pooling Layers\n",
    "\n",
    "A pooling layer reduces dimensionality (reducing the size of each feature map) and \"compresses\" information by combining several output elements. Two common functions used for pooling are:\n",
    "- Average pooling: calculating the average value for each patch on the feature map\n",
    "- Max pooling: calculating the maximum value for each patch of the feature map\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool1D\n",
    "\n",
    "```python\n",
    "# Example\n",
    "tf.keras.layers.MaxPool1D(pool_size=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "72ngF_beiaV9"
   },
   "source": [
    "### Activation Layers/Functions\n",
    "\n",
    "An activation function looks at each \"neuron\" in your neural network and determines whether it should be activated (fired) or not, based on the relevancy of the neuron's input to the modelâ€™s predictions. Some different activation functions you could look at are:\n",
    "- softmax https://www.tensorflow.org/api_docs/python/tf/keras/layers/Softmax\n",
    "- sigmoid https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid\n",
    "- softplus https://www.tensorflow.org/api_docs/python/tf/keras/activations/softplus\n",
    "- relu https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU\n",
    "\n",
    "```python\n",
    "# Example\n",
    "tf.keras.layers.Softmax()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zCdxM6HDqR1F"
   },
   "source": [
    "### Optimation Functions\n",
    "\n",
    "Optimation functions\n",
    "- Adam https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n",
    "  - Adam is computationally efficient, has little memory requirement, and is well suited for problems that are large in terms of data/parameter.\n",
    "- Adagrad https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adagrad\n",
    "  - Adagrad is an optimizer that is best used for sparse data. Some of its benefits are that it converges more quickly and doesn't need manual adjustment of the hyperparameter \"learning rate\".\n",
    "- SGD https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD\n",
    "  - SGD is a stochastic gradient descent and momentum optimizer. SGD essentially helps gradient vectors move down loss functions towards the minimum point, leading to faster \"converging\".\n",
    "- RMSprop https://keras.io/api/optimizers/rmsprop/\n",
    "  - As you may already know, the learning rate regulates how much the model \n",
    "can change based on the estimated error (which occurs every time the model's weights are updated). Instead of treating the learning rate as a hyperparamter, RMSprop is an optimization technique that uses relies on a changing, adaptive learning rate.\n",
    "\n",
    "```python\n",
    "# Example code\n",
    "l_r = .001 \n",
    "tf.keras.optimizers.SGD(learning_rate=l_r)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ldbularZ3cCW"
   },
   "source": [
    "## Putting Together Your Neural Network\n",
    "\n",
    "Now you will experiment with adding different layers to your neural network. We've added some guiding comments to give you a place to start and test out, but we also strongly encourage you to go through all the documetation and do some googling as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qMp_z7W9vZV4"
   },
   "outputs": [],
   "source": [
    "# Once you've gone through all the tests play around with these rate alues to see if you can increase your accuracy\n",
    "l_r = .001 \n",
    "d_r = 0.6\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(8, input_shape=(3,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8OQKRfNjBWGC"
   },
   "source": [
    "### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BneaEDk-BWj2"
   },
   "outputs": [],
   "source": [
    "# Run this cell as it is\n",
    "model.add(tf.keras.layers.Dense(8))\n",
    "model.add(tf.keras.layers.Dense(8))\n",
    "\n",
    "# output dimension needs to be number of classes in order for each to get a score\n",
    "model.add(tf.keras.layers.Dense(5))\n",
    "\n",
    "# Now skip down to the section that compiles and trains your model and run those cells.\n",
    "# Check the pseudo-test accuracy and see how well the bare minimum performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djwaQho7_xBt"
   },
   "source": [
    "### Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o2MQzNEa_ViW"
   },
   "outputs": [],
   "source": [
    "# Add activation layer here\n",
    "model.add(tf.keras.layers.Dense(8))\n",
    "# Add activation layer here\n",
    "model.add(tf.keras.layers.Dense(8))\n",
    "# Add activation layer here \n",
    "\n",
    "# output dimension needs to be number of classes in order for each to get a score\n",
    "model.add(tf.keras.layers.Dense(5))\n",
    "\n",
    "# Now skip down to the section that compiles and trains your model and re-run those cells.\n",
    "# What do you notice about the testing/Validation accuracy after Test 2 in comparison to Test 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M_pr4L_0Bzt_"
   },
   "source": [
    "### Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pb-7sVYkB11c"
   },
   "outputs": [],
   "source": [
    "# Add activation layer here\n",
    "model.add(tf.keras.layers.Dense(8))\n",
    "# Add activation layer here\n",
    "model.add(tf.keras.layers.Dense(8))\n",
    "# Add activation layer here \n",
    "\n",
    "# output dimension needs to be number of classes in order for each to get a score\n",
    "model.add(tf.keras.layers.Dense(5))\n",
    "\n",
    "# Add dropout layer here\n",
    "\n",
    "# Now skip down to the section that compiles and trains your model and re-run those cells.\n",
    "# What do you notice about the testing/Validation accuracy after Test 2 in comparison to Test 1 & 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YwjD-MgMDHFV"
   },
   "source": [
    "### Test 4\n",
    "\n",
    "Now go back down to the cell where you compiled your model, and this time change the optimizer. It's been set to Adam by default but as we showed you above, there are other functions that you can test out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qhna7lr5Dm56"
   },
   "source": [
    "## Compiling and Training Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XB1qNpsoASGv"
   },
   "outputs": [],
   "source": [
    "model.compile(loss = tf.keras.losses.categorical_crossentropy, \n",
    "              optimizer = tf.keras.optimizers.Adam(learning_rate=l_r),\n",
    "              metrics = ['accuracy'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tj1lwAY7BJPk"
   },
   "source": [
    "Fit Model to Data, Specify Number of Epochs and Batch Size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uF2RT0eGBIlD"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "batch_size = 100\n",
    "\n",
    "training_set = training_set.batch(batch_size) #set batch size\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for signals, labels in training_set:\n",
    "        tr_loss, tr_accuracy = model.train_on_batch(signals, labels)\n",
    "    val_loss, val_accuracy = model.evaluate(X_data_val, y_labels_val)\n",
    "    print(('Epoch #%d\\t Training Loss: %.2f\\tTraining Accuracy: %.2f\\t'\n",
    "         'Validation Loss: %.2f\\tValidation Accuracy: %.2f')\n",
    "         % (epoch + 1, tr_loss, tr_accuracy,\n",
    "         val_loss, val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4GipGyEkBQdj"
   },
   "outputs": [],
   "source": [
    "#Check Performance on Test Set\n",
    "test_loss, test_accuracy = model.evaluate(X_data_test, y_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "maTNvEQpbkoS"
   },
   "source": [
    "Now modify the existing model even more, and try to find the highest and appropriate testing and validation accuracy!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NeuralNetworks_Exercises.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
